# -*- coding: utf-8 -*-
"""[3_2] negative log likelihood

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yxGjKUK_WZ1kUiOILEIia1OnvQBiE2II
"""

import numpy as np
import matplotlib.pyplot as plt

import torch

"""# MNIST dataset"""

import tensorflow as tf

(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()

x_train = torch.tensor(x_train.reshape(60000, 784)/255, dtype=torch.float32)
x_test = torch.tensor(x_test.reshape(10000, 784)/255, dtype=torch.float32)

y_train = torch.tensor(y_train, dtype=torch.long)
y_test = torch.tensor(y_test, dtype=torch.int64)

# w = torch.randn(784, 10)
# b = torch.randn(10)

w = torch.randn(784, 10, requires_grad=True)
b = torch.randn(10, requires_grad=True)

x_train @ w + b

def log_softmax(x):
    return x - x.exp().sum(-1, keepdim=True).log()

def model(xb):
    return log_softmax(xb @ w + b)

"""# Maximum Likelihood (= minimize the negative log likelihood)"""

y_train

pred = x_train @ w + b
pred

"""## Ideal situtation

0 0 0 0 0 1 0 0 0 0 -> 5

1 0 0 0 0 0 0 0 0 0 -> 0

0 0 0 1 0 0 0 0 0 0 -> 4

...

0 0 0 0 0 0 0 0 1 0 -> 8

"""

pred[range(y_train.shape[0]),y_train]

pred[range(10), y_train[0:10]]

def nll(pred, target):
    return -pred[range(target.shape[0]), target].mean()

loss_fn = nll

pred = model(x_train)

nll(pred, y_train)

def accuracy(out, yb):
    preds = torch.argmax(out, dim=1)
    return (preds == yb).float().mean()



"""# Manual Training"""

pred = model(x_train)
loss = loss_fn(pred, y_train)

loss

loss.backward()

#w.grad

#b.grad

with torch.no_grad():
    w -= w.grad*0.1
    b -= b.grad*0.1
    w.grad.zero_()
    b.grad.zero_()

"""# Training"""

epochs = 100

for epoch in range(epochs):

    pred = model(x_train)
    loss = loss_fn(pred, y_train)
    if epoch % 10 == 0:
        print(loss)

    loss.backward()
    with torch.no_grad():
        w -= w.grad * 0.1
        b -= b.grad * 0.1
        w.grad.zero_()
        b.grad.zero_()

accuracy(model(x_train), y_train)

accuracy(model(x_test), y_test)

np.exp(-0.0615)

"""# Mini-batch"""

lr = 0.1  # learning rate
epochs = 100  # how many epochs to train for
bs = 64
n, c = x_train.shape

for epoch in range(epochs):
    for i in range((n - 1) // bs + 1):
        start_i = i * bs
        end_i = start_i + bs
        xb = x_train[start_i:end_i]
        yb = y_train[start_i:end_i]
        pred = model(xb)
        loss = loss_fn(pred, yb)

        loss.backward()
        with torch.no_grad():
            w -= w.grad * lr
            b -= b.grad * lr
            w.grad.zero_()
            b.grad.zero_()

    if epoch % 10 == 0:
            print(loss)

pred

accuracy(model(x_train), y_train)

accuracy(model(x_test), y_test)

np.exp(-0.0829)

"""# Refactoring with torch.nn"""

import torch.nn.functional as F

loss_fn = F.cross_entropy

def model(xb):
    return xb @ w + b

w = torch.randn(784, 10, requires_grad=True)
b = torch.randn(10, requires_grad=True)

lr = 0.1  # learning rate
epochs = 100  # how many epochs to train for
bs = 64
n, c = x_train.shape

for epoch in range(epochs):
    for i in range((n - 1) // bs + 1):
        start_i = i * bs
        end_i = start_i + bs
        xb = x_train[start_i:end_i]
        yb = y_train[start_i:end_i]
        pred = model(xb)
        loss = loss_fn(pred, yb)

        loss.backward()
        with torch.no_grad():
            w -= w.grad * lr
            b -= b.grad * lr
            w.grad.zero_()
            b.grad.zero_()

    if epoch % 10 == 0:
            print(loss)

accuracy(model(x_train), y_train)

