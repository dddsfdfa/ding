# -*- coding: utf-8 -*-
"""[4_2] What is torch.nn really?

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1H0R68qlht5Lyz_3rQKRrGdlREescoBhU
"""

import numpy as np

import torch

"""# MNIST dataset"""

import tensorflow as tf

(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()

x_train = torch.tensor(x_train.reshape(60000, 784)/255, dtype=torch.float32)
x_test = torch.tensor(x_test.reshape(10000, 784)/255, dtype=torch.float32)

y_train = torch.tensor(y_train, dtype=torch.long)
y_test = torch.tensor(y_test, dtype=torch.int64)

"""### Model, Parameters, Loss Function"""

import torch.nn.functional as F

w = torch.randn(784, 10, requires_grad=True)
b = torch.randn(10, requires_grad=True)

loss_fn = F.cross_entropy

def model(xb):
    return xb @ w + b

"""### Hyperparameters"""

bs = 64  # batch size
lr = 0.1  # learning rate
epochs = 10 # number of training

n, c = x_train.shape

(n-1) // bs + 1

for epoch in range(epochs):
    for i in range((n - 1) // bs + 1):
        start_i = i * bs
        end_i = start_i + bs
        xb = x_train[start_i:end_i]
        yb = y_train[start_i:end_i]
        pred = model(xb)
        loss = loss_fn(pred, yb)

        loss.backward()
        with torch.no_grad():
            w -= w.grad * lr
            b -= b.grad * lr
            w.grad.zero_()
            b.grad.zero_()

print(loss)

np.exp(-0.0687)

(torch.argmax(model(x_train), dim=-1) == y_train).float().mean()

y_train

def accuracy(out, yb):
    preds = torch.argmax(out, dim=1)
    return (preds == yb).float().mean()

accuracy(model(x_train), y_train)

torch.argmax(model(x_train), dim=1)

"""# Refactor using `nn.Module`"""

from torch import nn

class MyModel(nn.Module):

    def __init__(self):
        super().__init__()
        self.w = nn.Parameter(torch.randn(784,10))
        self.b = nn.Parameter(torch.randn(10))

    def forward(self, x):
        return x @ self.w + self.b

model = MyModel()

model

loss = loss_fn(model(x_train), y_train)
loss

loss.backward()

with torch.no_grad():
    for p in model.parameters():
        p -= p.grad * lr
    model.zero_grad()

def fit():

    for epoch in range(epochs):
        for i in range((n - 1) // bs + 1):
            start_i = i * bs
            end_i = start_i + bs
            xb = x_train[start_i:end_i]
            yb = y_train[start_i:end_i]
            pred = model(xb)
            loss = loss_fn(pred, yb)

            loss.backward()
            with torch.no_grad():
                for p in model.parameters():
                    p -= p.grad * lr
                model.zero_grad()

    print(loss)

fit()

np.exp(-0.0437)

accuracy(model(x_test), y_test)

"""# Refactoring using `nn.Linear`"""

class MyModel(nn.Module):

    def __init__(self):
        super().__init__()
        self.linear = nn.Linear(784, 10)

    def forward(self, x):
        return self.linear(x)

model = MyModel()
loss_fn(model(x_train), y_train)

np.exp(-2.3338)

fit()

np.exp(-0.0936)

loss_fn(model(x_test), y_test)

loss_fn(model(x_train), y_train)

np.exp(-0.2704)

accuracy(model(x_test), y_test)

"""# Refactoring using `torch.optim`"""

from torch import optim

opt = optim.Adam(model.parameters())

def fit():

    for epoch in range(epochs):
        for i in range((n - 1) // bs + 1):
            start_i = i * bs
            end_i = start_i + bs
            xb = x_train[start_i:end_i]
            yb = y_train[start_i:end_i]
            pred = model(xb)
            loss = loss_fn(pred, yb)

            loss.backward()
            opt.step()
            opt.zero_grad()

    print(loss)

fit()

np.exp(-0.0762)

"""# Refactor using `Dataset` and `DataLoader`"""

from torch.utils.data import TensorDataset
from torch.utils.data import DataLoader

train_ds = TensorDataset(x_train, y_train)

train_ds = TensorDataset(x_train, y_train)
train_dl = DataLoader(train_ds, batch_size=64, shuffle=True)

for i in train_dl:
    print(len(i[0][0]))

model = MyModel()
opt = optim.Adam(model.parameters())


def fit():

    for epoch in range(epochs):
        for xb, yb in train_dl:
            pred = model(xb)
            loss = loss_fn(pred, yb)

            loss.backward()
            opt.step()
            opt.zero_grad()

    print(loss)

fit()

accuracy(model(x_test), y_test)

"""# Summary"""

import torch
import torch.nn.functional as F
from torch.nn import Module, Linear
from torch import optim
from torch.utils.data import TensorDataset
from torch.utils.data import DataLoader

import tensorflow as tf

(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
x_train = torch.tensor(x_train.reshape(60000, 784)/255, dtype=torch.float32)
x_test = torch.tensor(x_test.reshape(10000, 784)/255, dtype=torch.float32)

y_train = torch.tensor(y_train, dtype=torch.long)
y_test = torch.tensor(y_test, dtype=torch.int64)

class MyModel(Module):

    def __init__(self):
        super().__init__()
        self.linear = Linear(784, 10)

    def forward(self, x):
        return self.linear(x)

train_ds = TensorDataset(x_train, y_train)
train_dl = DataLoader(train_ds, batch_size=64)

model = MyModel()
opt = optim.Adam(model.parameters())
loss_fn = F.cross_entropy
epochs=10

def fit():

    for epoch in range(epochs):
        for xb, yb in train_dl:
            pred = model(xb)
            loss = loss_fn(pred, yb)

            loss.backward()
            opt.step()
            opt.zero_grad()

    print(loss)

fit()

