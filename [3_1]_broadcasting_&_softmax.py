# -*- coding: utf-8 -*-
"""[3_1] broadcasting & softmax

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sMvHBDa7tzp_dqROC7HLqmCz-nCveUJ1
"""

import numpy as np
import matplotlib.pyplot as plt

import torch

"""# MNIST dataset"""

import tensorflow as tf

(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()

x_train

type(x_train)

x_train.shape

x_train[0]

plt.imshow(x_train[1])
plt.show()

y_train

y_train.shape

x_test[0]

y_test

"""### Question

> ⚛️ What is the difference between train data and test data in machine learning?

# Regression / Classification
"""

x_train = torch.tensor(x_train.reshape(60000, 784)/255, dtype=torch.float32)
x_test = torch.tensor(x_test.reshape(10000, 784)/255, dtype=torch.float32)

y_train = torch.tensor(y_train, dtype=torch.long)
y_test = torch.tensor(y_test, dtype=torch.int64)

x_train.shape

"""## Math

- Simple version

\begin{align}
y = \sum w_i x_i + b
\end{align}

- Matrix version

\begin{align}
y = X_{??? \times 784} W_{784 \times 10} + b_{1 \times 10}
\end{align}

https://alexlenail.me/NN-SVG/
"""

w = torch.randn(784, 10)
b = torch.randn(10)

w

x_train @ w + b

"""# 10 outputs"""

x = torch.tensor([-19.17,  12.94, -28.08,  -1.85,  -4.68,  -8.67,  15.68,  -9.47,   7.62, -1.10])

"""## Softmax

> ⚛️ What is a softmax?

\begin{align}
\frac{e^{x_j}}{\sum e^{x_i}}
\end{align}
"""

x

np.exp(x)

x.exp() / x.exp().sum()

(x.exp() / x.exp().sum()).sum()

y = x_train @ w + b
y.shape

y.exp()

y.exp().shape

y.exp() / y.exp().sum()

"""## Practice"""

a = torch.randn(20,8)

a.exp()

a.exp().sum()

a.exp().shape

a.exp().sum(0)

a.exp().sum(axis=1)

a.exp().sum(axis=-1)

a.exp().sum(axis=-1).shape

a.exp() / a.exp().sum(axis=-1)

# a - a.exp().sum(axis=-1).log().unsqueeze(-1)

a.exp().sum(axis=-1, keepdim=True)

a.exp() / a.exp().sum(axis=-1, keepdim=True)

a.exp() / a.exp().sum(axis=-1)

"""# Broadcasting

> ⚛️ What is a broadcasting rule in pytorch?
"""

import torch

a = torch.tensor([1, 2, 3]) # (3,)
b = torch.tensor([[4], [5], [6]]) # (3, 1)
result = a + b

result

"""(1,3)
(3,1)
"""

import torch

a = torch.tensor([1, 2, 3])
b = torch.tensor(5)
result = a + b

result

a.shape

a.exp().shape

a.exp().sum(axis=1,keepdim=True).shape

a.exp().sum(axis=-1).unsqueeze(-1).shape

a.exp() / a.exp().sum(axis=1)

a.exp().sum(axis=-1, keepdim=True).shape

a.exp().shape

a - a.exp().sum(-1, keepdim=True).log()



def log_softmax(x):
    return x - x.exp().sum(-1, keepdim=True).log()

"""### Appendix"""

x = torch.tensor([1.0, 1.1, 10000.0])

x.exp()

x.max()

x - x.max()

(x - x.max()).exp()

