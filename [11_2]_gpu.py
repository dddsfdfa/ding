# -*- coding: utf-8 -*-
"""[11_2] GPU

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Vz9AhzvVY1RM7hdtv_5lEscDriFNQsgl
"""

import torch

# Check if CUDA (GPU) is available
if torch.cuda.is_available():
    print("CUDA is available. You can use the GPU!")
else:
    print("CUDA is not available. Using the CPU instead.")

# Specify the device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Example: Creating a tensor and moving it to GPU
tensor = torch.randn(3, 3).to(device)

# You can also use tensor.cuda() as a shorthand
tensor = torch.randn(3, 3).cuda() if torch.cuda.is_available() else torch.randn(3, 3)

# Assuming you have a model
model = YourModel()

# Move the model to GPU
model.to(device)

# Example for CUDA shorthand
model.cuda() if torch.cuda.is_available() else model.cpu()

# Assuming you have input data and target labels
inputs, labels = inputs.to(device), labels.to(device)

# Forward pass
outputs = model(inputs)

# Compute loss
loss = criterion(outputs, labels)

# Training loop
for epoch in range(num_epochs):
    for data, target in train_loader:
        data, target = data.to(device), target.to(device)

        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()  # Compute gradients
        optimizer.step()  # Update parameters